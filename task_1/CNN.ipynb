{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIiVW6u4BwVP",
        "outputId": "a665f330-5d58-43af-a0d8-46bac0f1a7f0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zClGZYDyBz4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c484ec8c-76b8-4d1c-a42d-b9a45f840c81"
      },
      "source": [
        "root_path = 'gdrive/My Drive/Colab Notebooks/ML/Task 1/'\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlEgEh9_B0Bg"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "from torchvision.transforms import transforms\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Create writer for tensorboard\n",
        "writer = SummaryWriter('runs/Scratch-CNN')\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXhUU8OXB0Gf",
        "outputId": "bdb25eac-b01c-4728-fed7-48d3157d406e"
      },
      "source": [
        "# Slight data augmentation and normalization for training\n",
        "# Just normalization for validation and test\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((256,256)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((256,256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((256,256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ])\n",
        "}\n",
        "\n",
        "#Create datasets in dictionary with for loop passing in dir names and adding transform to tensor and normalize\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(root_path, x),data_transforms[x])\n",
        "                  for x in ['train', 'val', 'test']}\n",
        "#Load datasets in same way shuffle and make batch size 4 this can be tuned, workers at 2 for colab\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'val', 'test']}\n",
        "#Similar process read in dataset sizes for accuracy later on\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
        "\n",
        "#Read in classes from train, could be any as long as all directories have all classes\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "#Change to GPU if available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Check what device is being used as well as dataset sizes\n",
        "print(device)\n",
        "print(dataset_sizes)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "{'train': 1287, 'val': 389, 'test': 412}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJP8Nm1T-NVJ"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self,num_classes=6):\n",
        "        super(ConvNet,self).__init__()\n",
        "        #Conv2d extract features\n",
        "        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
        "        #BN1 change to out_chanels\n",
        "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
        "        #RELU linear activation function\n",
        "        self.relu1=nn.ReLU()\n",
        "        #Maxpool2d extract features\n",
        "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        #2nd Conv2d extract features\n",
        "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
        "        #2nd relu linear activation function\n",
        "        self.relu2=nn.ReLU()\n",
        "        \n",
        "        \n",
        "        #3rd conv2d extract features\n",
        "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
        "        #3rd bn change to out_chanels\n",
        "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
        "        #3rd relu linear activation function\n",
        "        self.relu3=nn.ReLU()\n",
        "        \n",
        "        #linear layer for output for classification\n",
        "        self.fc=nn.Linear(in_features=128 * 128 * 32,out_features=num_classes)\n",
        "      \n",
        "    #Feed forward function\n",
        "    def forward(self,input):\n",
        "      #input for conv is output of next and so on\n",
        "        output=self.conv1(input)\n",
        "        output=self.bn1(output)\n",
        "        output=self.relu1(output)\n",
        "        output=self.pool(output)\n",
        "        output=self.conv2(output)\n",
        "        output=self.relu2(output)\n",
        "        output=self.conv3(output)\n",
        "        output=self.bn3(output)\n",
        "        output=self.relu3(output)\n",
        "        #Output same as in features\n",
        "        output=output.view(-1,32*128*128)\n",
        "        #Final Out for classification\n",
        "        output=self.fc(output)\n",
        "        return output"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-mxJhlGb3PM"
      },
      "source": [
        "#Train model function again more comments\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs, device):\n",
        "  \"\"\"\n",
        "  Function for model training.\n",
        "\n",
        "  Args:\n",
        "    model: Model to be trained\n",
        "    criterion: Optimization criterion loss function\n",
        "    optimizer: Optimizer to use for training\n",
        "    scheduler: Instance of torch.optim.lr_scheduler\n",
        "    num_epochs: Number of epochs\n",
        "    device: Device to run the training on, CUDA or CPU on colab.\n",
        "  \"\"\"\n",
        "  #Time taken to train start\n",
        "  since = time.time()\n",
        "\n",
        "  #Copy current weights \n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  #Set accuracy\n",
        "  best_acc = 0.0\n",
        "  \n",
        "  #For loop for training\n",
        "  for epoch in range(num_epochs):\n",
        "    #Printing epochs\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "      if phase == 'train':\n",
        "        model.train()  # Set model to training mode\n",
        "      else:\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "      #Set running loss and correct count\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "\n",
        "      # Iterate over data.\n",
        "      for inputs, labels in dataloaders[phase]:\n",
        "        #Send to device for training\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          outputs = model(inputs)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          # backward + optimize only if in training phase\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "      #Step each train\n",
        "      if phase == 'train':\n",
        "        scheduler.step()\n",
        "      #Calculate loss and accuracy\n",
        "      epoch_loss = running_loss / dataset_sizes[phase]\n",
        "      epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "      #Print loss accuracy and phase\n",
        "      print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "      #Send to tensorboard to be plotted\n",
        "      if phase == 'train':\n",
        "          writer.add_scalar('Training Loss', epoch_loss/1000, epoch * len(dataloaders['train']))\n",
        "          writer.add_scalar('Training Accuracy', epoch_acc, epoch)\n",
        "      else:\n",
        "        writer.add_scalar('Validaiton Loss', epoch_loss/1000, epoch * len(dataloaders['val']))\n",
        "        writer.add_scalar('Validation Accuracy', epoch_acc, epoch)\n",
        "\n",
        "      # deep copy the model when decent score\n",
        "      if phase == 'val' and epoch_acc > best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print()\n",
        "  #When complete calc time taken\n",
        "  time_elapsed = time.time() - since\n",
        "  #Print time taken and validation accuracy \n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "  print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "  # load best model weights and send back trained model\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giMMZaazPTtA"
      },
      "source": [
        "#Call model to device and class number\n",
        "model_conv=ConvNet(num_classes=6).to(device)\n",
        "#Set number of features to the num of features model will look for\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "#Set feautres value and send num of classes\n",
        "model_conv.fc = nn.Linear(num_ftrs, 6)\n",
        "#Set to device\n",
        "model_conv = model_conv.to(device)\n",
        "#Create loss function \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Create optimiser for model\n",
        "optimizer_conv = optim.Adam(model_conv.fc.parameters(), lr=0.001)\n",
        "\n",
        "#Set schedular, decay learning rate by a factor of 0.1 every 5 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=5, gamma=0.3)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x_iY4gvyC1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a27942c-81a5-4ae1-82c9-eb6fc0ddc024"
      },
      "source": [
        "#Train model\n",
        "model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler,\n",
        "                             num_epochs=25, device=device)\n",
        "#Prune model to make lighter for actual deployment\n",
        "for name, module in model_conv.named_modules():\n",
        "    # prune 20% of connections in all 2D-conv layers \n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
        "        prune.remove(module, 'weight')\n",
        "    # prune 40% of connections in all linear layers \n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
        "        prune.remove(module, 'weight')\n",
        "\n",
        "#Call test data\n",
        "images, labels = next(iter(dataloaders['test']))\n",
        "#Send images to device to test and send to tensorboard same as model\n",
        "images = images.to(device)\n",
        "#Send to tensorboard\n",
        "writer.add_graph(model_conv, images)\n",
        "#Time to test model\n",
        "print(\"Testing model\")\n",
        "#Set confusion matric up as 6x6 (num of classes)\n",
        "confusion_matrix = torch.zeros(6, 6)\n",
        "#For loop for test and assigning to confusion matrix for multi class\n",
        "with torch.no_grad():\n",
        "    for i, (images, labels) in enumerate(dataloaders['test']):\n",
        "        #Set images and labels to same device as model\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        #Pass model image\n",
        "        outputs = model_conv(images)\n",
        "        #Compare model prediction with actual\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        #For loop to add to confusion matrix\n",
        "        for t, p in zip(labels.view(-1), pred.view(-1)):\n",
        "            confusion_matrix[t.long(), p.long()] +=1\n",
        "#Print matrix created\n",
        "print(confusion_matrix)\n",
        "#Find each class accuracy\n",
        "accuracy_individ = confusion_matrix.diag()/confusion_matrix.sum(1)\n",
        "#Print each class accuracy\n",
        "print(accuracy_individ)\n",
        "\n",
        "#Plot figure for easier understanding of model\n",
        "plt.figure(figsize=(15,10))\n",
        "#Set to dataframe for seaborn heatmap\n",
        "df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names).astype(int)\n",
        "#Call heatmap with dataframe\n",
        "heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
        "#Set colours and labels for display\n",
        "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=15)\n",
        "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=15)\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "#Save for later use\n",
        "conf = root_path + \"Conf_matrix_Scratch-CNN.png\"\n",
        "plt.savefig(conf)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "train Loss: 74.3700 Acc: 0.4359\n",
            "val Loss: 56.9752 Acc: 0.6195\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "train Loss: 51.2279 Acc: 0.6472\n",
            "val Loss: 45.4776 Acc: 0.6504\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "train Loss: 40.1430 Acc: 0.7475\n",
            "val Loss: 57.6695 Acc: 0.6555\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "train Loss: 36.9870 Acc: 0.7646\n",
            "val Loss: 70.6972 Acc: 0.6272\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "train Loss: 27.1535 Acc: 0.8050\n",
            "val Loss: 52.2905 Acc: 0.6761\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "train Loss: 9.6842 Acc: 0.9153\n",
            "val Loss: 52.7390 Acc: 0.6838\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "train Loss: 9.4867 Acc: 0.9262\n",
            "val Loss: 39.1895 Acc: 0.7455\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "train Loss: 5.9851 Acc: 0.9464\n",
            "val Loss: 44.0215 Acc: 0.7506\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "train Loss: 3.9490 Acc: 0.9549\n",
            "val Loss: 34.9964 Acc: 0.7789\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU0BhMQ3K0Ly"
      },
      "source": [
        "#convert accuracies to numpy\n",
        "accuracy_indi = accuracy_individ.numpy()\n",
        "#Convert matrix to numpy\n",
        "matrix = confusion_matrix.numpy()\n",
        "#Total up each class verical from matrix\n",
        "totals = np.sum(matrix, axis = 1)\n",
        "#Total up each class horizontal from matrix\n",
        "fp_total = np.sum(matrix, axis = 0)\n",
        "\n",
        "\n",
        "overall_total = sum(totals + fp_total)\n",
        "#Evaluation metrics for loop\n",
        "for i in range(6):\n",
        "  total = totals[i]\n",
        "  TP = total * accuracy_indi[i]\n",
        "  FN = total - TP\n",
        "  FP = fp_total[i] - TP\n",
        "  TN = overall_total -(total + fp_total[i])\n",
        "  Accuracy = (TP + TN)/ (TP + FP + TN + FN)\n",
        "  Precision = TP / (TP + FP)\n",
        "  Recall = TP / (TP + FN)\n",
        "  Specificity = TN / (TN + FP)\n",
        "  F1 = (2*TP) / (2*TP + FP + FN)\n",
        "  MCC = ((TP * TN) - (FP *FN)) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN) )\n",
        "  Error_rate = (FP + FN)/ (TP + FP + TN + FN) \n",
        "  Sensitivity = TP / (TP +FN)\n",
        "  print(class_names[i].capitalize())\n",
        "  print(\"True Positive value = \" + str(TP))\n",
        "  print(\"False Negative value = \" + str(FN))\n",
        "  print(\"False Positive value = \" + str(FP))\n",
        "  print(\"True Negative value = \" + str(TN))\n",
        "  print(\"Accuracy = \" + str(Accuracy))\n",
        "  print(\"Precision = \" + str(Precision))\n",
        "  print(\"Recall = \" + str(Recall))\n",
        "  print(\"Specificity = \" + str(Specificity))\n",
        "  print(\"F1 Score = \" + str(F1))\n",
        "  print(\"MCC = \" + str(MCC))\n",
        "  print(\"Error Rate = \" + str(Error_rate))\n",
        "  print(\"Sensitivity = \" + str(Sensitivity))\n",
        "  print()\n",
        "\n",
        "#Save the model for later use to be deployed using torchserve\n",
        "s = torch.jit.script(model_conv)\n",
        "torch.jit.save(s, root_path + 'Scratch-CNN.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtxII7ubHFwJ"
      },
      "source": [
        "!tensorboard dev upload \\\n",
        "  --logdir  runs/Scratch-CNN\\\n",
        "  --name \"Scratch CNN Model\" \\\n",
        "  --description \"Relevant information on model found here\" \\\n",
        "  --one_shot"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}